<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-23T08:18:20+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "mo"
}
-->
# AI代理在生產環境中的可觀測性與評估

隨著AI代理從實驗性原型轉向實際應用，理解其行為、監控其性能以及系統性地評估其輸出變得至關重要。

## 學習目標

完成本課程後，您將了解如何：
- 掌握代理可觀測性與評估的核心概念
- 提升代理性能、成本效益及有效性的技術
- 系統性地評估您的AI代理的內容與方法
- 控制AI代理部署到生產環境時的成本
- 為使用AutoGen構建的代理進行儀器化

目標是讓您具備將“黑箱”代理轉化為透明、可管理且可靠系統的知識。

_**注意：** 部署安全且值得信賴的AI代理非常重要。請參考[構建值得信賴的AI代理](./06-building-trustworthy-agents/README.md)課程。_

## 跟蹤與跨度

可觀測性工具如[Langfuse](https://langfuse.com/)或[Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry)通常將代理運行表示為跟蹤與跨度。

- **跟蹤**表示從開始到結束的完整代理任務（例如處理用戶查詢）。
- **跨度**是跟蹤中的個別步驟（例如調用語言模型或檢索數據）。

![Langfuse中的跟蹤樹](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

如果缺乏可觀測性，AI代理可能像“黑箱”一樣——其內部狀態和推理過程不透明，難以診斷問題或優化性能。有了可觀測性，代理就成為“玻璃箱”，提供透明性，這對建立信任並確保其按預期運行至關重要。

## 為什麼可觀測性在生產環境中至關重要

將AI代理轉移到生產環境會帶來一系列新的挑戰和需求。可觀測性不再是“可有可無”，而是一項關鍵能力：

* **調試與根本原因分析**：當代理失敗或產生意外輸出時，可觀測性工具提供所需的跟蹤以定位錯誤來源。這在涉及多次LLM調用、工具交互和條件邏輯的複雜代理中特別重要。
* **延遲與成本管理**：AI代理通常依賴LLM和其他按次計費的外部API。可觀測性允許精確跟蹤這些調用，幫助識別過於緩慢或昂貴的操作。這使得團隊能夠優化提示、選擇更高效的模型或重新設計工作流程，以管理運營成本並確保良好的用戶體驗。
* **信任、安全與合規**：在許多應用中，確保代理行為安全且符合道德至關重要。可觀測性提供代理行動和決策的審計記錄。這可用於檢測和緩解問題，例如提示注入、生成有害內容或處理個人身份信息（PII）不當。例如，您可以審查跟蹤以了解代理為何提供某個響應或使用特定工具。
* **持續改進循環**：可觀測性數據是迭代開發過程的基礎。通過監控代理在實際環境中的表現，團隊可以識別改進領域，收集數據以微調模型並驗證更改的影響。這創造了一個反饋循環，線上評估的生產洞察反饋到線下實驗和改進，從而逐步提升代理性能。

## 關鍵指標追蹤

為了監控和理解代理行為，需要追蹤一系列指標和信號。雖然具體指標可能因代理的用途而異，但有些是普遍重要的。

以下是可觀測性工具監控的一些常見指標：

**延遲：** 代理響應速度如何？長時間等待會對用戶體驗產生負面影響。您應通過跟蹤代理運行來測量任務和個別步驟的延遲。例如，一個代理在所有模型調用上花費20秒，可以通過使用更快的模型或並行運行模型調用來加速。

**成本：** 每次代理運行的費用是多少？AI代理依賴按令牌計費的LLM調用或外部API。頻繁的工具使用或多次提示可能迅速增加成本。例如，如果代理調用LLM五次僅為了微小的質量改進，您需要評估成本是否合理，或者是否可以減少調用次數或使用更便宜的模型。實時監控還可以幫助識別意外的峰值（例如，因錯誤導致的過度API循環）。

**請求錯誤：** 代理失敗了多少次請求？這可能包括API錯誤或工具調用失敗。為了使代理在生產環境中更具韌性，您可以設置備援或重試。例如，如果LLM提供商A宕機，您可以切換到LLM提供商B作為備份。

**用戶反饋：** 實施直接用戶評估提供有價值的洞察。這可以包括明確的評分（👍好評/👎差評，⭐1-5星）或文字評論。持續的負面反饋應引起您的注意，因為這表明代理未按預期工作。

**隱性用戶反饋：** 用戶行為即使沒有明確評分也能提供間接反饋。這可以包括立即重新措辭問題、重複查詢或點擊重試按鈕。例如，如果您發現用戶反復提出相同問題，這表明代理未按預期工作。

**準確性：** 代理生成正確或理想輸出的頻率如何？準確性的定義可能不同（例如，解決問題的正確性、信息檢索的準確性、用戶滿意度）。第一步是定義代理成功的標準。您可以通過自動檢查、評估分數或任務完成標籤來追蹤準確性。例如，將跟蹤標記為“成功”或“失敗”。

**自動評估指標：** 您還可以設置自動評估。例如，您可以使用LLM對代理的輸出進行評分，例如是否有幫助、準確或不準確。還有一些開源庫可以幫助您評分代理的不同方面。例如，[RAGAS](https://docs.ragas.io/)用於RAG代理或[LLM Guard](https://llm-guard.com/)用於檢測有害語言或提示注入。

實際操作中，這些指標的組合能提供AI代理健康狀況的最佳覆蓋。在本章的[示例筆記本](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)中，我們將展示這些指標在實際案例中的樣子，但首先，我們將學習典型的評估工作流程。

## 為代理進行儀器化

為了收集跟蹤數據，您需要對代碼進行儀器化。目標是儀器化代理代碼以發出可被可觀測性平台捕獲、處理和可視化的跟蹤和指標。

**OpenTelemetry (OTel)：** [OpenTelemetry](https://opentelemetry.io/)已成為LLM可觀測性的行業標準。它提供了一套API、SDK和工具，用於生成、收集和導出遙測數據。

有許多儀器化庫可以包裹現有的代理框架，並使其易於將OpenTelemetry跨度導出到可觀測性工具。以下是使用[OpenLit儀器化庫](https://github.com/openlit/openlit)為AutoGen代理進行儀器化的示例：

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

本章的[示例筆記本](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)將演示如何為您的AutoGen代理進行儀器化。

**手動創建跨度：** 儘管儀器化庫提供了良好的基線，但通常需要更詳細或自定義的信息。您可以手動創建跨度以添加自定義應用邏輯。更重要的是，它們可以通過自定義屬性（也稱為標籤或元數據）豐富自動或手動創建的跨度。這些屬性可以包括業務特定數據、中間計算或任何可能對調試或分析有用的上下文，例如`user_id`、`session_id`或`model_version`。

以下是使用[Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3)手動創建跟蹤和跨度的示例：

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## 代理評估

可觀測性提供指標，但評估是分析這些數據（並執行測試）以確定AI代理的性能以及如何改進的過程。換句話說，一旦您擁有這些跟蹤和指標，如何利用它們來判斷代理並做出決策？

定期評估很重要，因為AI代理通常是非確定性的，並且可能會演變（通過更新或模型行為漂移）——如果沒有評估，您無法知道您的“智能代理”是否真的在做好工作，或者是否出現了退化。

AI代理的評估分為兩類：**線下評估**和**線上評估**。兩者都很有價值，並且相輔相成。我們通常從線下評估開始，因為這是部署任何代理的最低必要步驟。

### 線下評估

![Langfuse中的數據集項目](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

這涉及在受控環境中評估代理，通常使用測試數據集，而不是實時用戶查詢。您使用精心策劃的數據集，知道預期輸出或正確行為，然後在這些數據集上運行代理。

例如，如果您構建了一個數學文字題代理，您可能有一個[測試數據集](https://huggingface.co/datasets/gsm8k)，包含100個問題及其已知答案。線下評估通常在開發期間進行（並且可以成為CI/CD管道的一部分），以檢查改進或防止退化。其優勢在於**可重複，並且由於有真實答案，您可以獲得清晰的準確性指標**。您還可以模擬用戶查詢並測量代理的響應是否符合理想答案，或使用上述自動指標。

線下評估的主要挑戰是確保您的測試數據集全面且保持相關性——代理可能在固定測試集上表現良好，但在生產中遇到非常不同的查詢。因此，您應該保持測試集的更新，加入新的邊界案例和反映真實場景的示例。混合使用小型“煙霧測試”案例和大型評估集是有用的：小型集用於快速檢查，大型集用於更廣泛的性能指標。

### 線上評估

![可觀測性指標概覽](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

這指的是在實時、真實環境中評估代理，即在生產中實際使用期間進行評估。線上評估涉及監控代理在真實用戶交互中的性能並持續分析結果。

例如，您可能追蹤成功率、用戶滿意度分數或其他指標以監控實時流量。線上評估的優勢在於它**捕捉到您可能在實驗室環境中無法預料的情況**——您可以觀察模型隨時間漂移（如果代理的有效性隨輸入模式的變化而下降），並捕捉測試數據中未包含的意外查詢或情況。它提供了代理在真實環境中的行為的真實畫面。

線上評估通常涉及收集隱性和顯性用戶反饋，如前所述，並可能運行影子測試或A/B測試（即新版本代理與舊版本並行運行以進行比較）。挑戰在於為實時交互獲得可靠的標籤或分數可能很棘手——您可能需要依賴用戶反饋或下游指標（例如用戶是否點擊結果）。

### 結合兩者

線上和線下評估並非互斥；它們高度互補。線上監控的洞察（例如代理在某些新類型用戶查詢中表現不佳）可以用於擴充和改進線下測試數據集。反之，在線下測試中表現良好的代理可以更有信心地部署並在線上進行監控。

事實上，許多團隊採用一個循環：

_線下評估 -> 部署 -> 線上監控 -> 收集新的失敗案例 -> 添加到線下數據集 -> 改進代理 -> 重複_。

## 常見問題

在將AI代理部署到生產環境時，您可能會遇到各種挑戰。以下是一些常見問題及其可能的解決方案：

| **問題**    | **可能的解決方案**   |
| ------------- | ------------------ |
| AI代理無法一致地執行任務 | - 改進提供給AI代理的提示；明確目標。<br>- 確定是否可以將任務分解為子任務並由多個代理處理。 |
| AI代理陷入連續循環 | - 確保您設置了明確的終止條件，讓代理知道何時停止過程。<br>- 對於需要推理和規劃的複雜任務，使用專門針對推理任務的大型模型。 |
| AI代理工具調用表現不佳 | - 在代理系統外部測試並驗證工具的輸出。 |

- 精煉已定義的參數、提示及工具命名。  |
| 多代理系統表現不一致 | - 精煉每個代理的提示，確保它們具體且彼此區分明確。<br>- 建立一個層級系統，使用「路由」或控制代理來判斷哪個代理是正確的選擇。 |

許多這些問題可以透過觀察性工具更有效地識別。我們之前討論的追蹤和指標有助於精準定位代理工作流程中問題發生的地方，使得除錯和優化更加高效。

## 成本管理

以下是一些管理部署 AI 代理至生產環境成本的策略：

**使用較小的模型：** 小型語言模型（SLM）在某些代理使用案例中表現良好，並能顯著降低成本。如前所述，建立一個評估系統來比較小型模型與大型模型的性能，是了解 SLM 在您的使用案例中表現如何的最佳方式。考慮將 SLM 用於較簡單的任務，例如意圖分類或參數提取，而將大型模型保留給需要複雜推理的任務。

**使用路由模型：** 一個類似的策略是使用多樣化的模型和大小。您可以使用 LLM/SLM 或無伺服器函數根據複雜性將請求路由至最適合的模型。這不僅能降低成本，還能確保在適合的任務上保持性能。例如，將簡單的查詢路由至較小、速度較快的模型，僅在需要複雜推理的任務上使用昂貴的大型模型。

**緩存回應：** 識別常見的請求和任務，並在它們進入代理系統之前提供回應，是減少類似請求量的好方法。您甚至可以實施一個流程，使用更基本的 AI 模型來識別請求與緩存請求的相似程度。這種策略可以顯著降低常見問題或常用工作流程的成本。

## 實際應用

在[本節的範例筆記本](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)中，我們將看到如何使用觀察性工具來監控和評估代理的例子。

## 前一課程

[元認知設計模式](../09-metacognition/README.md)

## 下一課程

[MCP](../11-mcp/README.md)

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。