<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-23T09:00:04+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "id"
}
-->
# Agen AI di Produksi: Observabilitas & Evaluasi

Saat agen AI beralih dari prototipe eksperimental ke aplikasi dunia nyata, kemampuan untuk memahami perilaku mereka, memantau kinerja mereka, dan mengevaluasi output mereka secara sistematis menjadi sangat penting.

## Tujuan Pembelajaran

Setelah menyelesaikan pelajaran ini, Anda akan mengetahui/memahami:
- Konsep inti tentang observabilitas dan evaluasi agen
- Teknik untuk meningkatkan kinerja, biaya, dan efektivitas agen
- Apa yang harus dievaluasi dan bagaimana mengevaluasi agen AI Anda secara sistematis
- Cara mengontrol biaya saat menerapkan agen AI ke produksi
- Cara menginstrumentasi agen yang dibangun dengan AutoGen

Tujuannya adalah untuk membekali Anda dengan pengetahuan untuk mengubah agen "kotak hitam" Anda menjadi sistem yang transparan, dapat dikelola, dan dapat diandalkan.

_**Catatan:** Penting untuk menerapkan Agen AI yang aman dan dapat dipercaya. Lihat juga pelajaran [Membangun Agen AI yang Dapat Dipercaya](./06-building-trustworthy-agents/README.md)._

## Jejak dan Rentang

Alat observabilitas seperti [Langfuse](https://langfuse.com/) atau [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) biasanya merepresentasikan proses agen sebagai jejak dan rentang.

- **Jejak** merepresentasikan tugas agen secara keseluruhan dari awal hingga akhir (seperti menangani permintaan pengguna).
- **Rentang** adalah langkah-langkah individu dalam jejak tersebut (seperti memanggil model bahasa atau mengambil data).

![Pohon jejak di Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Tanpa observabilitas, agen AI dapat terasa seperti "kotak hitam" - keadaan internal dan penalarannya tidak terlihat, sehingga sulit untuk mendiagnosis masalah atau mengoptimalkan kinerja. Dengan observabilitas, agen menjadi "kotak kaca," memberikan transparansi yang penting untuk membangun kepercayaan dan memastikan mereka beroperasi sesuai yang diharapkan.

## Mengapa Observabilitas Penting di Lingkungan Produksi

Transisi agen AI ke lingkungan produksi memperkenalkan serangkaian tantangan dan kebutuhan baru. Observabilitas bukan lagi sekadar "tambahan yang bagus," tetapi menjadi kemampuan yang sangat penting:

*   **Debugging dan Analisis Akar Masalah**: Ketika agen gagal atau menghasilkan output yang tidak terduga, alat observabilitas menyediakan jejak yang diperlukan untuk mengidentifikasi sumber kesalahan. Ini sangat penting dalam agen yang kompleks yang mungkin melibatkan banyak panggilan LLM, interaksi alat, dan logika bersyarat.
*   **Manajemen Latensi dan Biaya**: Agen AI sering mengandalkan LLM dan API eksternal lainnya yang dikenakan biaya per token atau per panggilan. Observabilitas memungkinkan pelacakan yang tepat atas panggilan ini, membantu mengidentifikasi operasi yang terlalu lambat atau mahal. Ini memungkinkan tim untuk mengoptimalkan prompt, memilih model yang lebih efisien, atau merancang ulang alur kerja untuk mengelola biaya operasional dan memastikan pengalaman pengguna yang baik.
*   **Kepercayaan, Keamanan, dan Kepatuhan**: Dalam banyak aplikasi, penting untuk memastikan bahwa agen berperilaku dengan aman dan etis. Observabilitas menyediakan jejak audit atas tindakan dan keputusan agen. Ini dapat digunakan untuk mendeteksi dan mengatasi masalah seperti injeksi prompt, pembuatan konten berbahaya, atau penanganan informasi pribadi yang salah. Misalnya, Anda dapat meninjau jejak untuk memahami mengapa agen memberikan respons tertentu atau menggunakan alat tertentu.
*   **Siklus Perbaikan Berkelanjutan**: Data observabilitas adalah dasar dari proses pengembangan iteratif. Dengan memantau bagaimana agen bekerja di dunia nyata, tim dapat mengidentifikasi area untuk perbaikan, mengumpulkan data untuk penyempurnaan model, dan memvalidasi dampak perubahan. Ini menciptakan siklus umpan balik di mana wawasan produksi dari evaluasi online menginformasikan eksperimen offline dan penyempurnaan, yang mengarah pada peningkatan kinerja agen secara progresif.

## Metrik Utama yang Harus Dilacak

Untuk memantau dan memahami perilaku agen, berbagai metrik dan sinyal harus dilacak. Meskipun metrik spesifik mungkin bervariasi berdasarkan tujuan agen, beberapa metrik bersifat universal.

Berikut adalah beberapa metrik paling umum yang dipantau oleh alat observabilitas:

**Latensi:** Seberapa cepat agen merespons? Waktu tunggu yang lama berdampak negatif pada pengalaman pengguna. Anda harus mengukur latensi untuk tugas dan langkah individu dengan melacak proses agen. Misalnya, agen yang membutuhkan waktu 20 detik untuk semua panggilan model dapat dipercepat dengan menggunakan model yang lebih cepat atau menjalankan panggilan model secara paralel.

**Biaya:** Berapa biaya per proses agen? Agen AI mengandalkan panggilan LLM yang dikenakan biaya per token atau API eksternal. Penggunaan alat yang sering atau banyak prompt dapat dengan cepat meningkatkan biaya. Misalnya, jika agen memanggil LLM lima kali untuk peningkatan kualitas yang kecil, Anda harus menilai apakah biaya tersebut sepadan atau apakah Anda dapat mengurangi jumlah panggilan atau menggunakan model yang lebih murah. Pemantauan waktu nyata juga dapat membantu mengidentifikasi lonjakan tak terduga (misalnya, bug yang menyebabkan loop API berlebihan).

**Kesalahan Permintaan:** Berapa banyak permintaan yang gagal dilakukan agen? Ini dapat mencakup kesalahan API atau kegagalan panggilan alat. Untuk membuat agen Anda lebih tangguh terhadap hal ini di produksi, Anda dapat mengatur fallback atau retry. Misalnya, jika penyedia LLM A tidak tersedia, Anda beralih ke penyedia LLM B sebagai cadangan.

**Umpan Balik Pengguna:** Implementasi evaluasi langsung dari pengguna memberikan wawasan berharga. Ini dapat mencakup penilaian eksplisit (ğŸ‘jempol atas/ğŸ‘bawah, â­1-5 bintang) atau komentar teks. Umpan balik negatif yang konsisten harus menjadi peringatan karena ini adalah tanda bahwa agen tidak bekerja seperti yang diharapkan.

**Umpan Balik Pengguna Implisit:** Perilaku pengguna memberikan umpan balik tidak langsung bahkan tanpa penilaian eksplisit. Ini dapat mencakup pengulangan pertanyaan segera, permintaan ulang, atau mengklik tombol coba lagi. Misalnya, jika Anda melihat pengguna berulang kali mengajukan pertanyaan yang sama, ini adalah tanda bahwa agen tidak bekerja seperti yang diharapkan.

**Akurasi:** Seberapa sering agen menghasilkan output yang benar atau diinginkan? Definisi akurasi bervariasi (misalnya, kebenaran pemecahan masalah, akurasi pengambilan informasi, kepuasan pengguna). Langkah pertama adalah mendefinisikan seperti apa keberhasilan untuk agen Anda. Anda dapat melacak akurasi melalui pemeriksaan otomatis, skor evaluasi, atau label penyelesaian tugas. Misalnya, menandai jejak sebagai "berhasil" atau "gagal."

**Metrik Evaluasi Otomatis:** Anda juga dapat mengatur evaluasi otomatis. Misalnya, Anda dapat menggunakan LLM untuk menilai output agen, misalnya apakah itu membantu, akurat, atau tidak. Ada juga beberapa pustaka open source yang membantu Anda menilai berbagai aspek agen. Misalnya, [RAGAS](https://docs.ragas.io/) untuk agen RAG atau [LLM Guard](https://llm-guard.com/) untuk mendeteksi bahasa berbahaya atau injeksi prompt.

Dalam praktiknya, kombinasi dari metrik-metrik ini memberikan cakupan terbaik atas kesehatan agen AI. Dalam [notebook contoh](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) bab ini, kami akan menunjukkan bagaimana metrik-metrik ini terlihat dalam contoh nyata, tetapi pertama-tama, kita akan mempelajari bagaimana alur kerja evaluasi yang khas terlihat.

## Instrumentasi Agen Anda

Untuk mengumpulkan data jejak, Anda perlu menginstrumentasi kode Anda. Tujuannya adalah untuk menginstrumentasi kode agen agar menghasilkan jejak dan metrik yang dapat ditangkap, diproses, dan divisualisasikan oleh platform observabilitas.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) telah muncul sebagai standar industri untuk observabilitas LLM. Ini menyediakan serangkaian API, SDK, dan alat untuk menghasilkan, mengumpulkan, dan mengekspor data telemetri.

Ada banyak pustaka instrumentasi yang membungkus kerangka kerja agen yang ada dan memudahkan untuk mengekspor rentang OpenTelemetry ke alat observabilitas. Berikut adalah contoh instrumentasi agen AutoGen dengan pustaka instrumentasi [OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

[Notebook contoh](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) dalam bab ini akan menunjukkan cara menginstrumentasi agen AutoGen Anda.

**Pembuatan Rentang Manual:** Meskipun pustaka instrumentasi menyediakan dasar yang baik, sering kali ada kasus di mana informasi yang lebih rinci atau khusus diperlukan. Anda dapat membuat rentang secara manual untuk menambahkan logika aplikasi khusus. Yang lebih penting, Anda dapat memperkaya rentang yang dibuat secara otomatis atau manual dengan atribut khusus (juga dikenal sebagai tag atau metadata). Atribut ini dapat mencakup data spesifik bisnis, perhitungan antara, atau konteks apa pun yang mungkin berguna untuk debugging atau analisis, seperti `user_id`, `session_id`, atau `model_version`.

Contoh pembuatan jejak dan rentang secara manual dengan [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Evaluasi Agen

Observabilitas memberi kita metrik, tetapi evaluasi adalah proses menganalisis data tersebut (dan melakukan pengujian) untuk menentukan seberapa baik agen AI bekerja dan bagaimana ia dapat ditingkatkan. Dengan kata lain, setelah Anda memiliki jejak dan metrik tersebut, bagaimana Anda menggunakannya untuk menilai agen dan membuat keputusan?

Evaluasi rutin penting karena agen AI sering kali bersifat non-deterministik dan dapat berkembang (melalui pembaruan atau perubahan perilaku model) â€“ tanpa evaluasi, Anda tidak akan tahu apakah "agen pintar" Anda benar-benar bekerja dengan baik atau justru mengalami kemunduran.

Ada dua kategori evaluasi untuk agen AI: **evaluasi offline** dan **evaluasi online**. Keduanya berharga dan saling melengkapi. Biasanya, kita memulai dengan evaluasi offline, karena ini adalah langkah minimum yang diperlukan sebelum menerapkan agen apa pun.

### Evaluasi Offline

![Item dataset di Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Ini melibatkan evaluasi agen dalam pengaturan yang terkendali, biasanya menggunakan dataset uji, bukan permintaan pengguna langsung. Anda menggunakan dataset yang dikurasi di mana Anda tahu apa output yang diharapkan atau perilaku yang benar, lalu menjalankan agen Anda pada dataset tersebut.

Misalnya, jika Anda membangun agen soal cerita matematika, Anda mungkin memiliki [dataset uji](https://huggingface.co/datasets/gsm8k) berisi 100 soal dengan jawaban yang diketahui. Evaluasi offline sering dilakukan selama pengembangan (dan dapat menjadi bagian dari pipeline CI/CD) untuk memeriksa perbaikan atau mencegah kemunduran. Keuntungannya adalah **dapat diulang dan Anda bisa mendapatkan metrik akurasi yang jelas karena Anda memiliki kebenaran dasar**. Anda juga dapat mensimulasikan permintaan pengguna dan mengukur respons agen terhadap jawaban ideal atau menggunakan metrik otomatis seperti yang dijelaskan di atas.

Tantangan utama dengan evaluasi offline adalah memastikan dataset uji Anda komprehensif dan tetap relevan â€“ agen mungkin berkinerja baik pada set uji tetap tetapi menghadapi permintaan yang sangat berbeda di produksi. Oleh karena itu, Anda harus terus memperbarui set uji dengan kasus tepi baru dan contoh yang mencerminkan skenario dunia nyata. Campuran kasus "smoke test" kecil dan set evaluasi yang lebih besar berguna: set kecil untuk pemeriksaan cepat dan set yang lebih besar untuk metrik kinerja yang lebih luas.

### Evaluasi Online

![Ikhtisar metrik observabilitas](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Ini mengacu pada evaluasi agen dalam lingkungan nyata, yaitu selama penggunaan langsung di produksi. Evaluasi online melibatkan pemantauan kinerja agen pada interaksi pengguna nyata dan menganalisis hasilnya secara terus-menerus.

Misalnya, Anda dapat melacak tingkat keberhasilan, skor kepuasan pengguna, atau metrik lainnya pada lalu lintas langsung. Keuntungan dari evaluasi online adalah **menangkap hal-hal yang mungkin tidak Anda antisipasi dalam pengaturan laboratorium** â€“ Anda dapat mengamati pergeseran model dari waktu ke waktu (jika efektivitas agen menurun seiring perubahan pola input) dan menangkap permintaan atau situasi tak terduga yang tidak ada dalam data uji Anda. Ini memberikan gambaran nyata tentang bagaimana agen berperilaku di dunia nyata.

Evaluasi online sering melibatkan pengumpulan umpan balik pengguna implisit dan eksplisit, seperti yang telah dibahas, dan mungkin menjalankan pengujian bayangan atau pengujian A/B (di mana versi baru agen berjalan secara paralel untuk dibandingkan dengan yang lama). Tantangannya adalah sulit mendapatkan label atau skor yang andal untuk interaksi langsung â€“ Anda mungkin mengandalkan umpan balik pengguna atau metrik hilir (seperti apakah pengguna mengklik hasilnya).

### Menggabungkan Keduanya

Evaluasi online dan offline tidak saling eksklusif; keduanya sangat saling melengkapi. Wawasan dari pemantauan online (misalnya, jenis permintaan pengguna baru di mana agen berkinerja buruk) dapat digunakan untuk menambah dan meningkatkan dataset uji offline. Sebaliknya, agen yang berkinerja baik dalam pengujian offline dapat lebih percaya diri diterapkan dan dipantau secara online.

Faktanya, banyak tim mengadopsi siklus:

_evaluasi offline -> terapkan -> pantau online -> kumpulkan kasus kegagalan baru -> tambahkan ke dataset offline -> perbaiki agen -> ulangi_.

## Masalah Umum

Saat Anda menerapkan agen AI ke produksi, Anda mungkin menghadapi berbagai tantangan. Berikut adalah beberapa masalah umum dan solusi potensialnya:

| **Masalah**    | **Solusi Potensial**   |
| ------------- | ------------------ |
| Agen AI tidak konsisten dalam menyelesaikan tugas | - Perbaiki prompt yang diberikan kepada Agen AI; pastikan tujuan jelas.<br>- Identifikasi di mana membagi tugas menjadi sub-tugas dan menanganinya dengan beberapa agen dapat membantu. |
| Agen AI terjebak dalam loop terus-menerus  | - Pastikan Anda memiliki syarat dan ketentuan penghentian yang jelas sehingga Agen tahu kapan harus menghentikan proses.<br>- Untuk tugas kompleks yang memerlukan penalaran dan perencanaan, gunakan model yang lebih besar yang khusus untuk tugas penalaran. |
| Panggilan alat oleh Agen AI tidak berjalan dengan baik   | - Uji dan validasi output alat di luar sistem agen. |

- Sempurnakan parameter yang telah ditentukan, prompt, dan penamaan alat.  
| Sistem Multi-Agen tidak berfungsi secara konsisten | - Sempurnakan prompt yang diberikan kepada setiap agen untuk memastikan bahwa prompt tersebut spesifik dan berbeda satu sama lain.<br>- Bangun sistem hierarkis menggunakan agen "routing" atau pengontrol untuk menentukan agen mana yang paling tepat. |

Banyak dari masalah ini dapat diidentifikasi dengan lebih efektif jika ada sistem observabilitas. Jejak dan metrik yang telah kita bahas sebelumnya membantu mengidentifikasi dengan tepat di mana masalah terjadi dalam alur kerja agen, sehingga proses debugging dan optimalisasi menjadi jauh lebih efisien.

## Mengelola Biaya

Berikut adalah beberapa strategi untuk mengelola biaya penerapan agen AI ke dalam produksi:

**Menggunakan Model yang Lebih Kecil:** Small Language Models (SLMs) dapat bekerja dengan baik pada beberapa kasus penggunaan agen tertentu dan secara signifikan mengurangi biaya. Seperti yang disebutkan sebelumnya, membangun sistem evaluasi untuk menentukan dan membandingkan kinerja dengan model yang lebih besar adalah cara terbaik untuk memahami seberapa baik SLM akan bekerja pada kasus penggunaan Anda. Pertimbangkan untuk menggunakan SLM untuk tugas-tugas yang lebih sederhana seperti klasifikasi intent atau ekstraksi parameter, sementara model yang lebih besar digunakan untuk penalaran yang lebih kompleks.

**Menggunakan Model Router:** Strategi serupa adalah menggunakan berbagai model dengan ukuran yang berbeda. Anda dapat menggunakan LLM/SLM atau fungsi serverless untuk mengarahkan permintaan berdasarkan kompleksitas ke model yang paling sesuai. Ini juga akan membantu mengurangi biaya sambil memastikan kinerja pada tugas yang tepat. Sebagai contoh, arahkan kueri sederhana ke model yang lebih kecil dan lebih cepat, dan hanya gunakan model besar yang mahal untuk tugas penalaran yang kompleks.

**Caching Respons:** Mengidentifikasi permintaan dan tugas yang umum serta menyediakan respons sebelum melalui sistem agen Anda adalah cara yang baik untuk mengurangi volume permintaan serupa. Anda bahkan dapat menerapkan alur untuk mengidentifikasi seberapa mirip suatu permintaan dengan permintaan yang telah di-cache menggunakan model AI yang lebih sederhana. Strategi ini dapat secara signifikan mengurangi biaya untuk pertanyaan yang sering diajukan atau alur kerja yang umum.

## Mari kita lihat bagaimana ini bekerja dalam praktik

Dalam [notebook contoh pada bagian ini](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb), kita akan melihat contoh bagaimana kita dapat menggunakan alat observabilitas untuk memantau dan mengevaluasi agen kita.

## Pelajaran Sebelumnya

[Metacognition Design Pattern](../09-metacognition/README.md)

## Pelajaran Selanjutnya

[MCP](../11-mcp/README.md)

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diperhatikan bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.