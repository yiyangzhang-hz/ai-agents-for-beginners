<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-23T08:38:08+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "br"
}
-->
# Agentes de IA em Produ√ß√£o: Observabilidade e Avalia√ß√£o

√Ä medida que os agentes de IA passam de prot√≥tipos experimentais para aplica√ß√µes no mundo real, torna-se essencial compreender seu comportamento, monitorar seu desempenho e avaliar sistematicamente seus resultados.

## Objetivos de Aprendizado

Ap√≥s concluir esta li√ß√£o, voc√™ saber√° como/entender√°:
- Conceitos fundamentais de observabilidade e avalia√ß√£o de agentes
- T√©cnicas para melhorar o desempenho, os custos e a efic√°cia dos agentes
- O que e como avaliar seus agentes de IA de forma sistem√°tica
- Como controlar os custos ao implantar agentes de IA em produ√ß√£o
- Como instrumentar agentes constru√≠dos com AutoGen

O objetivo √© equip√°-lo com o conhecimento necess√°rio para transformar seus agentes "caixa preta" em sistemas transparentes, gerenci√°veis e confi√°veis.

_**Nota:** √â importante implantar agentes de IA que sejam seguros e confi√°veis. Confira tamb√©m a li√ß√£o [Construindo Agentes de IA Confi√°veis](./06-building-trustworthy-agents/README.md)._

## Rastros e Spans

Ferramentas de observabilidade como [Langfuse](https://langfuse.com/) ou [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) geralmente representam as execu√ß√µes de agentes como rastros e spans.

- **Rastro** representa uma tarefa completa do agente do in√≠cio ao fim (como lidar com uma consulta de usu√°rio).
- **Spans** s√£o etapas individuais dentro do rastro (como chamar um modelo de linguagem ou recuperar dados).

![√Årvore de rastros no Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Sem observabilidade, um agente de IA pode parecer uma "caixa preta" - seu estado interno e racioc√≠nio s√£o opacos, dificultando o diagn√≥stico de problemas ou a otimiza√ß√£o do desempenho. Com a observabilidade, os agentes tornam-se "caixas de vidro", oferecendo transpar√™ncia vital para construir confian√ßa e garantir que operem conforme o esperado.

## Por que a Observabilidade √© Importante em Ambientes de Produ√ß√£o

A transi√ß√£o de agentes de IA para ambientes de produ√ß√£o apresenta um novo conjunto de desafios e requisitos. A observabilidade deixa de ser um "luxo" e se torna uma capacidade cr√≠tica:

*   **Depura√ß√£o e An√°lise de Causa Raiz**: Quando um agente falha ou gera um resultado inesperado, ferramentas de observabilidade fornecem os rastros necess√°rios para identificar a origem do erro. Isso √© especialmente importante em agentes complexos que podem envolver v√°rias chamadas de LLM, intera√ß√µes com ferramentas e l√≥gica condicional.
*   **Gerenciamento de Lat√™ncia e Custos**: Agentes de IA frequentemente dependem de LLMs e outras APIs externas que s√£o cobradas por token ou por chamada. A observabilidade permite o rastreamento preciso dessas chamadas, ajudando a identificar opera√ß√µes excessivamente lentas ou caras. Isso possibilita otimizar prompts, selecionar modelos mais eficientes ou redesenhar fluxos de trabalho para gerenciar custos operacionais e garantir uma boa experi√™ncia do usu√°rio.
*   **Confian√ßa, Seguran√ßa e Conformidade**: Em muitas aplica√ß√µes, √© importante garantir que os agentes se comportem de maneira segura e √©tica. A observabilidade fornece um registro de auditoria das a√ß√µes e decis√µes do agente. Isso pode ser usado para detectar e mitigar problemas como inje√ß√£o de prompts, gera√ß√£o de conte√∫do prejudicial ou o manuseio inadequado de informa√ß√µes pessoalmente identific√°veis (PII). Por exemplo, voc√™ pode revisar rastros para entender por que um agente forneceu uma determinada resposta ou utilizou uma ferramenta espec√≠fica.
*   **Ciclos de Melhoria Cont√≠nua**: Dados de observabilidade s√£o a base de um processo de desenvolvimento iterativo. Ao monitorar como os agentes se comportam no mundo real, as equipes podem identificar √°reas para melhoria, coletar dados para ajuste fino de modelos e validar o impacto das mudan√ßas. Isso cria um ciclo de feedback onde insights de produ√ß√£o a partir de avalia√ß√µes online informam experimentos offline e refinamentos, levando a um desempenho progressivamente melhor dos agentes.

## M√©tricas-Chave para Monitorar

Para monitorar e compreender o comportamento dos agentes, uma s√©rie de m√©tricas e sinais deve ser acompanhada. Embora as m√©tricas espec√≠ficas possam variar dependendo do prop√≥sito do agente, algumas s√£o universalmente importantes.

Aqui est√£o algumas das m√©tricas mais comuns monitoradas por ferramentas de observabilidade:

**Lat√™ncia:** Com que rapidez o agente responde? Tempos de espera longos impactam negativamente a experi√™ncia do usu√°rio. Voc√™ deve medir a lat√™ncia para tarefas e etapas individuais rastreando as execu√ß√µes do agente. Por exemplo, um agente que leva 20 segundos para todas as chamadas de modelo poderia ser acelerado usando um modelo mais r√°pido ou executando chamadas de modelo em paralelo.

**Custos:** Qual √© o custo por execu√ß√£o do agente? Agentes de IA dependem de chamadas de LLM cobradas por token ou APIs externas. O uso frequente de ferramentas ou m√∫ltiplos prompts pode aumentar rapidamente os custos. Por exemplo, se um agente chama um LLM cinco vezes para uma melhoria marginal na qualidade, voc√™ deve avaliar se o custo √© justificado ou se poderia reduzir o n√∫mero de chamadas ou usar um modelo mais barato. O monitoramento em tempo real tamb√©m pode ajudar a identificar picos inesperados (por exemplo, bugs causando loops excessivos de API).

**Erros de Requisi√ß√£o:** Quantas requisi√ß√µes o agente falhou? Isso pode incluir erros de API ou falhas em chamadas de ferramentas. Para tornar seu agente mais robusto contra esses problemas em produ√ß√£o, voc√™ pode configurar alternativas ou tentativas de repeti√ß√£o. Por exemplo, se o provedor de LLM A estiver fora do ar, voc√™ muda para o provedor de LLM B como backup.

**Feedback do Usu√°rio:** Implementar avalia√ß√µes diretas dos usu√°rios fornece insights valiosos. Isso pode incluir classifica√ß√µes expl√≠citas (üëçpositivo/üëénegativo, ‚≠ê1-5 estrelas) ou coment√°rios textuais. Feedback negativo consistente deve alert√°-lo, pois √© um sinal de que o agente n√£o est√° funcionando como esperado.

**Feedback Impl√≠cito do Usu√°rio:** Comportamentos dos usu√°rios fornecem feedback indireto mesmo sem classifica√ß√µes expl√≠citas. Isso pode incluir reformula√ß√µes imediatas de perguntas, consultas repetidas ou cliques em um bot√£o de tentativa novamente. Por exemplo, se voc√™ perceber que os usu√°rios repetidamente fazem a mesma pergunta, isso √© um sinal de que o agente n√£o est√° funcionando como esperado.

**Precis√£o:** Com que frequ√™ncia o agente gera sa√≠das corretas ou desej√°veis? As defini√ß√µes de precis√£o variam (por exemplo, corre√ß√£o na resolu√ß√£o de problemas, precis√£o na recupera√ß√£o de informa√ß√µes, satisfa√ß√£o do usu√°rio). O primeiro passo √© definir o que significa sucesso para o seu agente. Voc√™ pode rastrear a precis√£o por meio de verifica√ß√µes automatizadas, pontua√ß√µes de avalia√ß√£o ou r√≥tulos de conclus√£o de tarefas. Por exemplo, marcar rastros como "bem-sucedidos" ou "falhos".

**M√©tricas de Avalia√ß√£o Automatizada:** Voc√™ tamb√©m pode configurar avalia√ß√µes automatizadas. Por exemplo, pode usar um LLM para pontuar a sa√≠da do agente, avaliando se √© √∫til, precisa ou n√£o. Existem tamb√©m v√°rias bibliotecas de c√≥digo aberto que ajudam a pontuar diferentes aspectos do agente. Por exemplo, [RAGAS](https://docs.ragas.io/) para agentes RAG ou [LLM Guard](https://llm-guard.com/) para detectar linguagem prejudicial ou inje√ß√£o de prompts.

Na pr√°tica, uma combina√ß√£o dessas m√©tricas oferece a melhor cobertura da sa√∫de de um agente de IA. No [notebook de exemplo](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) deste cap√≠tulo, mostraremos como essas m√©tricas aparecem em exemplos reais, mas primeiro aprenderemos como √© um fluxo de trabalho t√≠pico de avalia√ß√£o.

## Instrumente seu Agente

Para coletar dados de rastreamento, voc√™ precisar√° instrumentar seu c√≥digo. O objetivo √© instrumentar o c√≥digo do agente para emitir rastros e m√©tricas que possam ser capturados, processados e visualizados por uma plataforma de observabilidade.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) emergiu como um padr√£o da ind√∫stria para observabilidade de LLM. Ele fornece um conjunto de APIs, SDKs e ferramentas para gerar, coletar e exportar dados de telemetria.

Existem muitas bibliotecas de instrumenta√ß√£o que envolvem frameworks de agentes existentes e facilitam a exporta√ß√£o de spans do OpenTelemetry para uma ferramenta de observabilidade. Abaixo est√° um exemplo de instrumenta√ß√£o de um agente AutoGen com a biblioteca de instrumenta√ß√£o [OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

O [notebook de exemplo](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) deste cap√≠tulo demonstrar√° como instrumentar seu agente AutoGen.

**Cria√ß√£o Manual de Spans:** Embora bibliotecas de instrumenta√ß√£o forne√ßam uma boa base, muitas vezes h√° casos em que informa√ß√µes mais detalhadas ou personalizadas s√£o necess√°rias. Voc√™ pode criar spans manualmente para adicionar l√≥gica de aplica√ß√£o personalizada. Mais importante, eles podem enriquecer spans criados automaticamente ou manualmente com atributos personalizados (tamb√©m conhecidos como tags ou metadados). Esses atributos podem incluir dados espec√≠ficos do neg√≥cio, c√°lculos intermedi√°rios ou qualquer contexto que possa ser √∫til para depura√ß√£o ou an√°lise, como `user_id`, `session_id` ou `model_version`.

Exemplo de cria√ß√£o de rastros e spans manualmente com o [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Avalia√ß√£o de Agentes

A observabilidade nos fornece m√©tricas, mas a avalia√ß√£o √© o processo de analisar esses dados (e realizar testes) para determinar qu√£o bem um agente de IA est√° se saindo e como ele pode ser melhorado. Em outras palavras, uma vez que voc√™ tenha esses rastros e m√©tricas, como us√°-los para julgar o agente e tomar decis√µes?

A avalia√ß√£o regular √© importante porque agentes de IA frequentemente s√£o n√£o determin√≠sticos e podem evoluir (por meio de atualiza√ß√µes ou mudan√ßas no comportamento do modelo) ‚Äì sem avalia√ß√£o, voc√™ n√£o saberia se seu "agente inteligente" est√° realmente cumprindo bem sua fun√ß√£o ou se regrediu.

Existem duas categorias de avalia√ß√µes para agentes de IA: **avalia√ß√£o offline** e **avalia√ß√£o online**. Ambas s√£o valiosas e se complementam. Geralmente come√ßamos com a avalia√ß√£o offline, pois este √© o passo m√≠nimo necess√°rio antes de implantar qualquer agente.

### Avalia√ß√£o Offline

![Itens de conjunto de dados no Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Isso envolve avaliar o agente em um ambiente controlado, geralmente usando conjuntos de dados de teste, e n√£o consultas de usu√°rios ao vivo. Voc√™ utiliza conjuntos de dados curados onde sabe qual √© o resultado esperado ou o comportamento correto e, em seguida, executa seu agente nesses dados.

Por exemplo, se voc√™ criou um agente para resolver problemas matem√°ticos, pode ter um [conjunto de dados de teste](https://huggingface.co/datasets/gsm8k) com 100 problemas e respostas conhecidas. A avalia√ß√£o offline √© frequentemente realizada durante o desenvolvimento (e pode fazer parte de pipelines de CI/CD) para verificar melhorias ou evitar regress√µes. O benef√≠cio √© que ela √© **repet√≠vel e voc√™ pode obter m√©tricas claras de precis√£o, j√° que possui a verdade de base**. Voc√™ tamb√©m pode simular consultas de usu√°rios e medir as respostas do agente em rela√ß√£o √†s respostas ideais ou usar m√©tricas automatizadas, como descrito acima.

O principal desafio da avalia√ß√£o offline √© garantir que seu conjunto de dados de teste seja abrangente e permane√ßa relevante ‚Äì o agente pode se sair bem em um conjunto de teste fixo, mas encontrar consultas muito diferentes em produ√ß√£o. Portanto, voc√™ deve manter os conjuntos de teste atualizados com novos casos extremos e exemplos que reflitam cen√°rios do mundo real. Uma combina√ß√£o de pequenos casos de "teste de fuma√ßa" e conjuntos de avalia√ß√£o maiores √© √∫til: conjuntos pequenos para verifica√ß√µes r√°pidas e maiores para m√©tricas de desempenho mais amplas.

### Avalia√ß√£o Online

![Vis√£o geral de m√©tricas de observabilidade](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Isso se refere √† avalia√ß√£o do agente em um ambiente ao vivo, no mundo real, ou seja, durante o uso real em produ√ß√£o. A avalia√ß√£o online envolve monitorar o desempenho do agente em intera√ß√µes reais com usu√°rios e analisar continuamente os resultados.

Por exemplo, voc√™ pode rastrear taxas de sucesso, pontua√ß√µes de satisfa√ß√£o do usu√°rio ou outras m√©tricas no tr√°fego ao vivo. A vantagem da avalia√ß√£o online √© que ela **captura coisas que voc√™ pode n√£o antecipar em um ambiente de laborat√≥rio** ‚Äì voc√™ pode observar o desvio do modelo ao longo do tempo (se a efic√°cia do agente diminuir √† medida que os padr√µes de entrada mudam) e identificar consultas ou situa√ß√µes inesperadas que n√£o estavam em seus dados de teste. Ela fornece uma imagem real de como o agente se comporta no mundo real.

A avalia√ß√£o online frequentemente envolve a coleta de feedback impl√≠cito e expl√≠cito dos usu√°rios, como discutido, e possivelmente a execu√ß√£o de testes sombra ou testes A/B (onde uma nova vers√£o do agente √© executada em paralelo para compara√ß√£o com a antiga). O desafio √© que pode ser dif√≠cil obter r√≥tulos ou pontua√ß√µes confi√°veis para intera√ß√µes ao vivo ‚Äì voc√™ pode depender de feedback do usu√°rio ou m√©tricas a jusante (como se o usu√°rio clicou no resultado).

### Combinando as duas

As avalia√ß√µes online e offline n√£o s√£o mutuamente exclusivas; elas se complementam muito bem. Insights do monitoramento online (por exemplo, novos tipos de consultas de usu√°rios onde o agente apresenta desempenho ruim) podem ser usados para aumentar e melhorar os conjuntos de dados de teste offline. Por outro lado, agentes que se saem bem em testes offline podem ser implantados e monitorados online com mais confian√ßa.

Na verdade, muitas equipes adotam um ciclo:

_avaliar offline -> implantar -> monitorar online -> coletar novos casos de falha -> adicionar ao conjunto de dados offline -> refinar o agente -> repetir_.

## Problemas Comuns

Ao implantar agentes de IA em produ√ß√£o, voc√™ pode encontrar v√°rios desafios. Aqui est√£o alguns problemas comuns e suas poss√≠veis solu√ß√µes:

| **Problema**    | **Solu√ß√£o Potencial**   |
| ------------- | ------------------ |
| Agente de IA n√£o realiza tarefas de forma consistente | - Refine o prompt dado ao agente de IA; seja claro nos objetivos.<br>- Identifique onde dividir as tarefas em subtarefas e deleg√°-las a m√∫ltiplos agentes pode ajudar. |
| Agente de IA entrando em loops cont√≠nuos  | - Certifique-se de que h√° termos e condi√ß√µes claros de t√©rmino para que o agente saiba quando parar o processo.<br>- Para tarefas complexas que exigem racioc√≠nio e planejamento, use um modelo maior especializado em tarefas de racioc√≠nio. |
| Chamadas de ferramentas do agente de IA n√£o est√£o funcionando bem   | - Teste e valide a sa√≠da da ferramenta fora do sistema do agente. |

- Refine os par√¢metros definidos, os prompts e a nomenclatura das ferramentas.  |
| Sistema Multi-Agente n√£o est√° funcionando de forma consistente | - Refine os prompts fornecidos a cada agente para garantir que sejam espec√≠ficos e distintos entre si.<br>- Construa um sistema hier√°rquico usando um agente "roteador" ou controlador para determinar qual agente √© o mais adequado. |

Muitos desses problemas podem ser identificados de forma mais eficaz com observabilidade implementada. Os rastreamentos e m√©tricas que discutimos anteriormente ajudam a identificar exatamente onde ocorrem os problemas no fluxo de trabalho dos agentes, tornando o processo de depura√ß√£o e otimiza√ß√£o muito mais eficiente.

## Gerenciando Custos

Aqui est√£o algumas estrat√©gias para gerenciar os custos de implanta√ß√£o de agentes de IA em produ√ß√£o:

**Usando Modelos Menores:** Modelos de Linguagem Pequenos (SLMs) podem ter um bom desempenho em certos casos de uso de agentes e reduzir√£o significativamente os custos. Como mencionado anteriormente, construir um sistema de avalia√ß√£o para determinar e comparar o desempenho em rela√ß√£o a modelos maiores √© a melhor maneira de entender como um SLM se sair√° no seu caso de uso. Considere usar SLMs para tarefas mais simples, como classifica√ß√£o de inten√ß√£o ou extra√ß√£o de par√¢metros, reservando modelos maiores para racioc√≠nios mais complexos.

**Usando um Modelo Roteador:** Uma estrat√©gia semelhante √© usar uma diversidade de modelos e tamanhos. Voc√™ pode usar um LLM/SLM ou fun√ß√£o serverless para direcionar solicita√ß√µes com base na complexidade para os modelos mais adequados. Isso tamb√©m ajudar√° a reduzir custos, garantindo desempenho nas tarefas certas. Por exemplo, direcione consultas simples para modelos menores e mais r√°pidos, e use modelos grandes e caros apenas para tarefas de racioc√≠nio complexo.

**Cache de Respostas:** Identificar solicita√ß√µes e tarefas comuns e fornecer as respostas antes que passem pelo seu sistema de agentes √© uma boa maneira de reduzir o volume de solicita√ß√µes semelhantes. Voc√™ pode at√© implementar um fluxo para identificar qu√£o semelhante uma solicita√ß√£o √© √†s solicita√ß√µes armazenadas em cache usando modelos de IA mais b√°sicos. Essa estrat√©gia pode reduzir significativamente os custos para perguntas frequentes ou fluxos de trabalho comuns.

## Vamos ver como isso funciona na pr√°tica

No [notebook de exemplo desta se√ß√£o](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb), veremos exemplos de como podemos usar ferramentas de observabilidade para monitorar e avaliar nosso agente.

## Aula Anterior

[Padr√£o de Design de Metacogni√ß√£o](../09-metacognition/README.md)

## Pr√≥xima Aula

[MCP](../11-mcp/README.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.