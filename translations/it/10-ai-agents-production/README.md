<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-23T08:39:31+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "it"
}
-->
# Agenti AI in Produzione: Osservabilit√† e Valutazione

Man mano che gli agenti AI passano da prototipi sperimentali ad applicazioni reali, diventa fondamentale comprendere il loro comportamento, monitorare le loro prestazioni e valutare sistematicamente i loro output.

## Obiettivi di Apprendimento

Dopo aver completato questa lezione, saprai come/avrai compreso:
- Concetti fondamentali di osservabilit√† e valutazione degli agenti
- Tecniche per migliorare le prestazioni, i costi e l'efficacia degli agenti
- Cosa e come valutare sistematicamente i tuoi agenti AI
- Come controllare i costi quando si distribuiscono agenti AI in produzione
- Come strumentare agenti costruiti con AutoGen

L'obiettivo √® fornirti le conoscenze necessarie per trasformare i tuoi agenti "scatola nera" in sistemi trasparenti, gestibili e affidabili.

_**Nota:** √à importante distribuire agenti AI sicuri e affidabili. Dai un'occhiata alla lezione [Costruire Agenti AI Affidabili](./06-building-trustworthy-agents/README.md)._

## Tracce e Spans

Gli strumenti di osservabilit√† come [Langfuse](https://langfuse.com/) o [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) solitamente rappresentano le esecuzioni degli agenti come tracce e spans.

- **Traccia** rappresenta un compito completo dell'agente dall'inizio alla fine (ad esempio, gestire una query dell'utente).
- **Spans** sono i singoli passaggi all'interno della traccia (ad esempio, chiamare un modello linguistico o recuperare dati).

![Albero delle tracce in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Senza osservabilit√†, un agente AI pu√≤ sembrare una "scatola nera" - il suo stato interno e il suo ragionamento sono opachi, rendendo difficile diagnosticare problemi o ottimizzare le prestazioni. Con l'osservabilit√†, gli agenti diventano "scatole di vetro", offrendo trasparenza essenziale per costruire fiducia e garantire che operino come previsto.

## Perch√© l'Osservabilit√† √® Importante negli Ambienti di Produzione

Il passaggio degli agenti AI agli ambienti di produzione introduce una nuova serie di sfide e requisiti. L'osservabilit√† non √® pi√π un "optional", ma una capacit√† critica:

*   **Debugging e Analisi delle Cause Radice**: Quando un agente fallisce o produce un output inaspettato, gli strumenti di osservabilit√† forniscono le tracce necessarie per individuare la fonte dell'errore. Questo √® particolarmente importante in agenti complessi che potrebbero coinvolgere molteplici chiamate LLM, interazioni con strumenti e logica condizionale.
*   **Gestione della Latenza e dei Costi**: Gli agenti AI spesso si affidano a LLM e altre API esterne che vengono fatturate per token o per chiamata. L'osservabilit√† consente di tracciare con precisione queste chiamate, aiutando a identificare operazioni eccessivamente lente o costose. Questo permette ai team di ottimizzare i prompt, selezionare modelli pi√π efficienti o riprogettare i flussi di lavoro per gestire i costi operativi e garantire una buona esperienza utente.
*   **Fiducia, Sicurezza e Conformit√†**: In molte applicazioni, √® importante garantire che gli agenti si comportino in modo sicuro ed etico. L'osservabilit√† fornisce una traccia di audit delle azioni e decisioni dell'agente. Questo pu√≤ essere utilizzato per rilevare e mitigare problemi come l'iniezione di prompt, la generazione di contenuti dannosi o la gestione impropria di informazioni personali identificabili (PII). Ad esempio, puoi esaminare le tracce per capire perch√© un agente ha fornito una certa risposta o utilizzato uno specifico strumento.
*   **Cicli di Miglioramento Continuo**: I dati di osservabilit√† sono la base di un processo di sviluppo iterativo. Monitorando come gli agenti si comportano nel mondo reale, i team possono identificare aree di miglioramento, raccogliere dati per il fine-tuning dei modelli e validare l'impatto delle modifiche. Questo crea un ciclo di feedback in cui gli insight di produzione derivati dalla valutazione online informano la sperimentazione offline e il perfezionamento, portando a prestazioni degli agenti progressivamente migliori.

## Metriche Chiave da Monitorare

Per monitorare e comprendere il comportamento degli agenti, √® necessario tracciare una serie di metriche e segnali. Sebbene le metriche specifiche possano variare in base allo scopo dell'agente, alcune sono universalmente importanti.

Ecco alcune delle metriche pi√π comuni che gli strumenti di osservabilit√† monitorano:

**Latenza:** Quanto velocemente risponde l'agente? Tempi di attesa lunghi influenzano negativamente l'esperienza utente. Dovresti misurare la latenza per i compiti e i singoli passaggi tracciando le esecuzioni dell'agente. Ad esempio, un agente che impiega 20 secondi per tutte le chiamate al modello potrebbe essere accelerato utilizzando un modello pi√π veloce o eseguendo le chiamate al modello in parallelo.

**Costi:** Qual √® il costo per esecuzione dell'agente? Gli agenti AI si affidano a chiamate LLM fatturate per token o API esterne. L'uso frequente di strumenti o pi√π prompt pu√≤ aumentare rapidamente i costi. Ad esempio, se un agente chiama un LLM cinque volte per un miglioramento marginale della qualit√†, devi valutare se il costo √® giustificato o se potresti ridurre il numero di chiamate o utilizzare un modello pi√π economico. Il monitoraggio in tempo reale pu√≤ anche aiutare a identificare picchi inaspettati (ad esempio, bug che causano loop API eccessivi).

**Errori di Richiesta:** Quante richieste ha fallito l'agente? Questo pu√≤ includere errori API o chiamate agli strumenti fallite. Per rendere il tuo agente pi√π robusto in produzione, puoi configurare fallback o retry. Ad esempio, se il provider LLM A √® inattivo, puoi passare al provider LLM B come backup.

**Feedback degli Utenti:** Implementare valutazioni dirette degli utenti fornisce preziosi insight. Questo pu√≤ includere valutazioni esplicite (üëçpollice in su/üëégi√π, ‚≠ê1-5 stelle) o commenti testuali. Feedback negativi costanti dovrebbero allertarti, poich√© indicano che l'agente non sta funzionando come previsto.

**Feedback Implicito degli Utenti:** I comportamenti degli utenti forniscono feedback indiretto anche senza valutazioni esplicite. Questo pu√≤ includere la riformulazione immediata delle domande, query ripetute o clic su un pulsante di retry. Ad esempio, se vedi che gli utenti pongono ripetutamente la stessa domanda, √® un segnale che l'agente non sta funzionando come previsto.

**Accuratezza:** Con quale frequenza l'agente produce output corretti o desiderabili? Le definizioni di accuratezza variano (ad esempio, correttezza nella risoluzione di problemi, accuratezza nel recupero di informazioni, soddisfazione dell'utente). Il primo passo √® definire cosa significa successo per il tuo agente. Puoi tracciare l'accuratezza tramite controlli automatizzati, punteggi di valutazione o etichette di completamento del compito. Ad esempio, contrassegnare le tracce come "riuscite" o "fallite".

**Metriche di Valutazione Automatica:** Puoi anche configurare valutazioni automatiche. Ad esempio, puoi utilizzare un LLM per valutare l'output dell'agente, ad esempio se √® utile, accurato o meno. Esistono anche diverse librerie open source che aiutano a valutare diversi aspetti dell'agente. Ad esempio, [RAGAS](https://docs.ragas.io/) per agenti RAG o [LLM Guard](https://llm-guard.com/) per rilevare linguaggio dannoso o iniezione di prompt.

In pratica, una combinazione di queste metriche offre la migliore copertura sulla salute di un agente AI. Nel [notebook di esempio](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) di questo capitolo, ti mostreremo come queste metriche appaiono in esempi reali, ma prima impareremo come appare un tipico flusso di lavoro di valutazione.

## Strumentare il tuo Agente

Per raccogliere dati di tracciamento, dovrai strumentare il tuo codice. L'obiettivo √® strumentare il codice dell'agente per emettere tracce e metriche che possono essere catturate, elaborate e visualizzate da una piattaforma di osservabilit√†.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) √® emerso come uno standard del settore per l'osservabilit√† degli LLM. Fornisce un set di API, SDK e strumenti per generare, raccogliere ed esportare dati di telemetria.

Esistono molte librerie di strumentazione che avvolgono i framework degli agenti esistenti e rendono facile esportare spans OpenTelemetry a uno strumento di osservabilit√†. Di seguito √® riportato un esempio di strumentazione di un agente AutoGen con la libreria di strumentazione [OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

Il [notebook di esempio](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) in questo capitolo dimostrer√† come strumentare il tuo agente AutoGen.

**Creazione Manuale di Spans:** Sebbene le librerie di strumentazione forniscano una buona base, ci sono spesso casi in cui sono necessarie informazioni pi√π dettagliate o personalizzate. Puoi creare spans manualmente per aggiungere logica applicativa personalizzata. Ancora pi√π importante, possono arricchire spans creati automaticamente o manualmente con attributi personalizzati (noti anche come tag o metadati). Questi attributi possono includere dati specifici del business, calcoli intermedi o qualsiasi contesto che potrebbe essere utile per il debugging o l'analisi, come `user_id`, `session_id` o `model_version`.

Esempio di creazione manuale di tracce e spans con il [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Valutazione degli Agenti

L'osservabilit√† ci fornisce metriche, ma la valutazione √® il processo di analisi di quei dati (e di esecuzione di test) per determinare quanto bene un agente AI sta performando e come pu√≤ essere migliorato. In altre parole, una volta che hai quelle tracce e metriche, come le usi per giudicare l'agente e prendere decisioni?

La valutazione regolare √® importante perch√© gli agenti AI sono spesso non deterministici e possono evolvere (attraverso aggiornamenti o comportamenti del modello che cambiano) ‚Äì senza valutazione, non sapresti se il tuo "agente intelligente" sta effettivamente svolgendo bene il suo lavoro o se √® regredito.

Ci sono due categorie di valutazioni per gli agenti AI: **valutazione offline** e **valutazione online**. Entrambe sono preziose e si completano a vicenda. Di solito si inizia con la valutazione offline, poich√© questo √® il minimo necessario prima di distribuire qualsiasi agente.

### Valutazione Offline

![Elementi del dataset in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Questa consiste nel valutare l'agente in un ambiente controllato, tipicamente utilizzando dataset di test, non query di utenti live. Si utilizzano dataset curati in cui si conosce l'output previsto o il comportamento corretto, e poi si esegue l'agente su questi.

Ad esempio, se hai costruito un agente per problemi di matematica, potresti avere un [dataset di test](https://huggingface.co/datasets/gsm8k) di 100 problemi con risposte conosciute. La valutazione offline viene spesso eseguita durante lo sviluppo (e pu√≤ far parte delle pipeline CI/CD) per verificare miglioramenti o proteggersi da regressioni. Il vantaggio √® che √® **ripetibile e puoi ottenere metriche di accuratezza chiare poich√© hai una verit√† di base**. Potresti anche simulare query degli utenti e misurare le risposte dell'agente rispetto a risposte ideali o utilizzare metriche automatizzate come descritto sopra.

La sfida principale con la valutazione offline √® garantire che il tuo dataset di test sia completo e rimanga rilevante ‚Äì l'agente potrebbe performare bene su un set di test fisso ma incontrare query molto diverse in produzione. Pertanto, dovresti mantenere i set di test aggiornati con nuovi casi limite ed esempi che riflettano scenari reali‚Äã. Una combinazione di piccoli casi di "smoke test" e set di valutazione pi√π ampi √® utile: piccoli set per controlli rapidi e set pi√π grandi per metriche di prestazione pi√π ampie‚Äã.

### Valutazione Online

![Panoramica delle metriche di osservabilit√†](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Questa si riferisce alla valutazione dell'agente in un ambiente reale, ovvero durante l'uso effettivo in produzione. La valutazione online implica il monitoraggio delle prestazioni dell'agente su interazioni reali degli utenti e l'analisi continua dei risultati.

Ad esempio, potresti tracciare tassi di successo, punteggi di soddisfazione degli utenti o altre metriche sul traffico live. Il vantaggio della valutazione online √® che **cattura cose che potresti non anticipare in un ambiente di laboratorio** ‚Äì puoi osservare il drift del modello nel tempo (se l'efficacia dell'agente si degrada man mano che i pattern di input cambiano) e rilevare query o situazioni inaspettate che non erano nel tuo dataset di test‚Äã. Fornisce un quadro reale di come l'agente si comporta nel mondo reale.

La valutazione online spesso implica la raccolta di feedback impliciti ed espliciti degli utenti, come discusso, e possibilmente l'esecuzione di test ombra o test A/B (dove una nuova versione dell'agente viene eseguita in parallelo per confrontarsi con la vecchia). La sfida √® che pu√≤ essere difficile ottenere etichette o punteggi affidabili per interazioni live ‚Äì potresti fare affidamento su feedback degli utenti o metriche a valle (ad esempio, l'utente ha cliccato sul risultato).

### Combinare le Due

Le valutazioni online e offline non si escludono a vicenda; si completano a vicenda. Gli insight derivati dal monitoraggio online (ad esempio, nuovi tipi di query degli utenti in cui l'agente performa male) possono essere utilizzati per arricchire e migliorare i dataset di test offline. Al contrario, gli agenti che performano bene nei test offline possono essere distribuiti e monitorati online con maggiore sicurezza.

Infatti, molti team adottano un ciclo:

_valutare offline -> distribuire -> monitorare online -> raccogliere nuovi casi di fallimento -> aggiungere al dataset offline -> perfezionare l'agente -> ripetere_.

## Problemi Comuni

Quando distribuisci agenti AI in produzione, potresti incontrare varie sfide. Ecco alcuni problemi comuni e le loro possibili soluzioni:

| **Problema**    | **Possibile Soluzione**   |
| ------------- | ------------------ |
| L'agente AI non esegue i compiti in modo coerente | - Affina il prompt dato all'agente AI; sii chiaro sugli obiettivi.<br>- Identifica dove suddividere i compiti in sottocompiti e gestirli con pi√π agenti pu√≤ essere utile. |
| L'agente AI entra in loop continui  | - Assicurati di avere termini e condizioni di terminazione chiari affinch√© l'agente sappia quando interrompere il processo.<br>- Per compiti complessi che richiedono ragionamento e pianificazione, utilizza un modello pi√π grande specializzato per compiti di ragionamento. |
| Le chiamate agli strumenti dell'agente AI non funzionano bene   | - Testa e valida l'output dello strumento al di fuori del sistema dell'agente. |

- Raffinare i parametri definiti, i prompt e la denominazione degli strumenti.  |
| Il sistema multi-agente non funziona in modo coerente | - Raffinare i prompt forniti a ciascun agente per garantire che siano specifici e distinti l'uno dall'altro.<br>- Costruire un sistema gerarchico utilizzando un agente "router" o controller per determinare quale agente sia quello corretto. |

Molti di questi problemi possono essere identificati in modo pi√π efficace con un sistema di osservabilit√† in atto. Le tracce e le metriche di cui abbiamo parlato in precedenza aiutano a individuare esattamente dove si verificano i problemi nel flusso di lavoro degli agenti, rendendo il debugging e l'ottimizzazione molto pi√π efficienti.

## Gestione dei costi

Ecco alcune strategie per gestire i costi di implementazione degli agenti AI in produzione:

**Utilizzo di modelli pi√π piccoli:** I Small Language Models (SLM) possono funzionare bene in alcuni casi d'uso agentici e ridurre significativamente i costi. Come menzionato in precedenza, costruire un sistema di valutazione per determinare e confrontare le prestazioni rispetto ai modelli pi√π grandi √® il modo migliore per capire quanto bene un SLM possa funzionare per il tuo caso d'uso. Considera di utilizzare gli SLM per compiti pi√π semplici come la classificazione delle intenzioni o l'estrazione di parametri, riservando i modelli pi√π grandi per ragionamenti complessi.

**Utilizzo di un modello router:** Una strategia simile consiste nell'usare una variet√† di modelli di dimensioni diverse. Puoi utilizzare un LLM/SLM o una funzione serverless per instradare le richieste in base alla complessit√† verso i modelli pi√π adatti. Questo aiuter√† a ridurre i costi garantendo al contempo prestazioni adeguate per i compiti giusti. Ad esempio, instrada le query semplici verso modelli pi√π piccoli e veloci, e utilizza modelli grandi e costosi solo per compiti di ragionamento complessi.

**Caching delle risposte:** Identificare richieste e compiti comuni e fornire le risposte prima che passino attraverso il tuo sistema agentico √® un buon modo per ridurre il volume di richieste simili. Puoi persino implementare un flusso per identificare quanto una richiesta sia simile alle richieste memorizzate nella cache utilizzando modelli AI pi√π basilari. Questa strategia pu√≤ ridurre significativamente i costi per domande frequenti o flussi di lavoro comuni.

## Vediamo come funziona in pratica

Nell'[esempio di notebook di questa sezione](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb), vedremo esempi di come possiamo utilizzare strumenti di osservabilit√† per monitorare e valutare il nostro agente.

## Lezione precedente

[Design Pattern di Metacognizione](../09-metacognition/README.md)

## Prossima lezione

[MCP](../11-mcp/README.md)

**Disclaimer**:  
Questo documento √® stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.